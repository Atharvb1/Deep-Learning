{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2827061",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6afef6",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61771261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96d248",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a6fd7",
   "metadata": {},
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228434c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2, \n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)  #fliping the images horizontally\n",
    "training_set = train_datagen.flow_from_directory(r'F:\\Files\\ Udemy DL\\Part 2 - Convolutional Neural Networks\\dataset\\training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "#use r before file path to avoid system can't find the path error\n",
    "#shear_range is used to augment images so that computer can see how humans see things from different angles\n",
    "#rescale- each image takes pixxel values between 1-255. it will apply feature scaling to each image by dividing it by 255\n",
    "#target size(64,64)- size of images when they feed into CNN\n",
    "#batch size=32 - How many images we want in each batch 32 is ideal\n",
    "#class_mode= binary - as we have binary outcome i.e. cat or dog so we chose binary if we have categorical choose category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6475695",
   "metadata": {},
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca02534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(r'F:\\Files\\ Udemy DL\\Part 2 - Convolutional Neural Networks\\dataset\\test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "#we only rescale the test set to avoid the information leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29097cf4",
   "metadata": {},
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb9708",
   "metadata": {},
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f43123",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796d922",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a56d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "#filters=32- no. of feature detectors\n",
    "#kernel size=3 - our feature detector is 3x3 matrix so we chose 3\n",
    "#input shape[64,64,3]- 64x64 as we have resize the images and 3 for coloured images 1 for black & white\n",
    "# we have to mention input shape only in the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edc138",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d117bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#MaxPool method\n",
    "#pool size=2- 2x2 matrix from feature map\n",
    "#strides=2- by how many rows matrix shifted\n",
    "#2x2 is ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e475b",
   "metadata": {},
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec33ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c242bf7",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b1f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())\n",
    "# 1 dimentional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a75ef",
   "metadata": {},
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1478ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "#units- no. of hidden neurons you want in this fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636360fb",
   "metadata": {},
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d37088",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "#use softmax if you have more than 2 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59b55d",
   "metadata": {},
   "source": [
    "### Part 3 - Training the CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b25a92",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc0fa683",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "#adam optimiser to perform stochastic gradient descent in order to reduce the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c593c",
   "metadata": {},
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d83dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 454s 2s/step - loss: 0.6607 - accuracy: 0.5969 - val_loss: 0.6153 - val_accuracy: 0.6660\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 237s 949ms/step - loss: 0.5895 - accuracy: 0.6894 - val_loss: 0.5474 - val_accuracy: 0.7260\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 220s 878ms/step - loss: 0.5526 - accuracy: 0.7131 - val_loss: 0.5963 - val_accuracy: 0.6790\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 188s 750ms/step - loss: 0.5122 - accuracy: 0.7406 - val_loss: 0.5166 - val_accuracy: 0.7400\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 169s 675ms/step - loss: 0.4960 - accuracy: 0.7580 - val_loss: 0.4719 - val_accuracy: 0.7705\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.4801 - accuracy: 0.7690 - val_loss: 0.4877 - val_accuracy: 0.7625\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 102s 410ms/step - loss: 0.4524 - accuracy: 0.7841 - val_loss: 0.5004 - val_accuracy: 0.7560\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 103s 411ms/step - loss: 0.4392 - accuracy: 0.7939 - val_loss: 0.4451 - val_accuracy: 0.7925\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 102s 407ms/step - loss: 0.4354 - accuracy: 0.7970 - val_loss: 0.4511 - val_accuracy: 0.7915\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 102s 410ms/step - loss: 0.4136 - accuracy: 0.8016 - val_loss: 0.4441 - val_accuracy: 0.7990\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.4003 - accuracy: 0.8144 - val_loss: 0.4405 - val_accuracy: 0.8020\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 104s 414ms/step - loss: 0.3929 - accuracy: 0.8206 - val_loss: 0.4233 - val_accuracy: 0.8050\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 106s 422ms/step - loss: 0.3779 - accuracy: 0.8255 - val_loss: 0.4158 - val_accuracy: 0.8110\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.3691 - accuracy: 0.8341 - val_loss: 0.4205 - val_accuracy: 0.8225\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 101s 403ms/step - loss: 0.3624 - accuracy: 0.8324 - val_loss: 0.4001 - val_accuracy: 0.8290\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 103s 410ms/step - loss: 0.3534 - accuracy: 0.8410 - val_loss: 0.3926 - val_accuracy: 0.8300\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.3362 - accuracy: 0.8522 - val_loss: 0.4456 - val_accuracy: 0.7940\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 102s 406ms/step - loss: 0.3313 - accuracy: 0.8568 - val_loss: 0.4120 - val_accuracy: 0.8285\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.3167 - accuracy: 0.8589 - val_loss: 0.4064 - val_accuracy: 0.8200\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 101s 406ms/step - loss: 0.3148 - accuracy: 0.8622 - val_loss: 0.4381 - val_accuracy: 0.8240\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 102s 408ms/step - loss: 0.2990 - accuracy: 0.8730 - val_loss: 0.3913 - val_accuracy: 0.8255\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 102s 407ms/step - loss: 0.2996 - accuracy: 0.8692 - val_loss: 0.4121 - val_accuracy: 0.8205\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 102s 408ms/step - loss: 0.2886 - accuracy: 0.8748 - val_loss: 0.4273 - val_accuracy: 0.8305\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 103s 410ms/step - loss: 0.2741 - accuracy: 0.8811 - val_loss: 0.4018 - val_accuracy: 0.8390\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 101s 406ms/step - loss: 0.2697 - accuracy: 0.8813 - val_loss: 0.3915 - val_accuracy: 0.8320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25e378fe4f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
    "#epochs=25- neiral network will train itself 25 times to improve the accuracy by applying backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc047e3",
   "metadata": {},
   "source": [
    "### Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a40ad617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'F:\\Files\\ Udemy DL\\Part 2 - Convolutional Neural Networks\\dataset\\single_prediction\\cat_or_dog_1.jpg',\n",
    "                            target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "#convert image to a 2D array\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "#add dimension corresponding to batch\n",
    "result = cnn.predict(test_image/255.0)\n",
    "training_set.class_indices\n",
    "#by printing training_set.class_indices you will know which class corresponds to which category i.e. cat=0 and dog=1\n",
    "if result[0][0] > 0.5:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c42e831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'F:\\Files\\ Udemy DL\\Part 2 - Convolutional Neural Networks\\dataset\\single_prediction\\cat_or_dog_2.jpg',\n",
    "                            target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcd5437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'F:\\Files\\ Udemy DL\\Part 2 - Convolutional Neural Networks\\dataset\\single_prediction\\download.jpg',\n",
    "                            target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e896e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
